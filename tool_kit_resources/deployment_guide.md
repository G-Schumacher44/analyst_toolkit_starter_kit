<p align="center">
  <img src="../repo_img/starterkit_banner.png" width="1000"/>
  <br>
  <em>Analyst Toolkit: Deployment Starterkitâ€” Deployment Guide</em>
</p>

# ğŸš€ Deployment Guide (Notebookâ€‘First)

This guide shows how to unzip the deployment bundle, scaffold a project, autoâ€‘ingest a dataset, infer starter configs locally (no AI, no network), and launch the notebook for Diagnostics and Validation.

---

## âœ… Prerequisites

- Conda installed and initialized in your shell (`conda --version`)
- Make available (`make --version`)
- Network access if youâ€™re not using offline installs

---

## ğŸ§© What the Bundle Contains

- `Makefile` (at repo root): convenient targets for setup, wiring data, config inference, packaging
- `deploy_toolkit/templates/**`: configuration, env, VS Code, and notebook templates
- `deploy_toolkit/scripts/bootstrap.sh`: notebookâ€‘first bootstrapper
- `deploy_toolkit/tool_kit_resources/**`: docs (this guide, usage, config)

---

## âš¡ Quick Start

1) Unzip the bundle at your project root (Makefile should be at `./Makefile`).
2) Add your CSV at repo root (e.g., `./my_data.csv`) or `./data/raw/`.
3) Activate Conda base:

```bash
conda activate base
```

4) Run setup (builds env, scaffolds repo, infers configs, wires dataset):

```bash
make setup PROJECT_NAME=<your_project_name>
```

Sample initial setup commands:

```bash
# Most common (auto-discovers a single CSV at root or data/raw and copies it into data/raw)
make setup PROJECT_NAME=my_project DATASET=auto

# Prompt to select when multiple CSVs are present
make setup PROJECT_NAME=my_project DATASET=prompt

# Explicit env/kernel names (optional override)
make setup ENV=deploy_toolkit KERNEL_NAME="Python (deploy_toolkit)" PROJECT_NAME=my_project DATASET=auto
```

Defaults in setup:
- `TARGET=.` (repo root)
- `ENV=<folder_name>`; `KERNEL_NAME="Python (<ENV>)"`
- `DATASET=auto` (uses single CSV; prompts if multiple)
- `INGEST=copy` (copies a root CSV into `data/raw/`)
- `GENERATE=1` (writes `config/generated/*`)

5) Launch the starter notebook:

```bash
conda activate <ENV>
make notebook
```

Then select kernel â€œPython (<ENV>)â€ and Run All.

---

## ğŸ”© What Setup Produces

- Scaffolds: `config/`, `data/{raw,processed,features}`, `exports/{joblib,plots,reports}`, `notebooks/`, `.vscode/`
- Copies: `.env`, `.gitignore`, `environment.yml`, `requirements.txt`, `README.md`
- Notebook: `notebooks/toolkit_template.ipynb` (titled with `PROJECT_NAME`)
- Wiring: `config/run_toolkit_config.yaml:pipeline_entry_path` set to your CSV
- Inference (local, privacyâ€‘safe): `config/generated/*` (validation, certification, outliers)
- Env + kernel: Conda env `<ENV>`, kernel â€œPython (<ENV>)â€

---

## ğŸ—‚ï¸ Data Ingestion Behavior

- If `data/raw/` contains exactly one CSV â†’ uses that automatically
- Else, if the project root contains exactly one CSV â†’ copies (or moves) it into `data/raw/` and wires it
- If multiple CSVs are found (root or `data/raw/`) â†’ prompts to select (unless `DATASET=/path.csv` is provided)
- Control ingestion with `INGEST=copy|move|none` (default `copy`)

Privacy: Inference/config generation is local Python code; no AI models or network calls.

---

## ğŸ§ª Notebookâ€‘First Flow

1) Run M01 Diagnostics â€” inspect schema, nulls, cardinality, skew
2) Run M02 Validation (soft) â€” tune expected columns/types/ranges in YAML
3) Iterate configs, reâ€‘run cells
4) Enable downstream modules when ready (normalization â†’ duplicates â†’ outliers â†’ handling â†’ imputation â†’ final audit â†’ certification strict)

---

## ğŸ› ï¸ Make Targets

Common targets:

- Setup (alias for full bootstrap):
  ```bash
  make setup PROJECT_NAME=<name>
  ```

- Wire dataset later (after setup):
  ```bash
  make wire-data DATASET=auto        # auto when exactly one CSV
  make wire-data DATASET=prompt      # interactive selection
  make wire-data DATASET=/full/path.csv
  ```

- Regenerate config suggestions (local):
  ```bash
  make configs INPUT=data/raw/your.csv MAX_UNIQUE=30 SAMPLE_ROWS=50000
  ```

- Launch notebook:
  ```bash
  conda activate <ENV>
  make notebook
  ```

- Package deployable zip:
  ```bash
  make package
  ```

- Clean any prior scaffolds inside `deploy_toolkit/` (keeps templates/scripts/resources):
  ```bash
  make clean-deploy-scaffold
  ```

---

## âš™ï¸ Variables

These can be supplied to any `make` command:

- `TARGET`: project root (default `.`)
- `ENV`: Conda environment (default = folder name)
- `KERNEL_NAME`: Jupyter kernel (default `Python (<ENV>)`)
- `DATASET`: `auto|prompt|/path.csv` (default `auto`)
- `INGEST`: `copy|move|none` (default `copy`) â€” how to ingest a root CSV into `data/raw/`
- `GENERATE`: `1|0` (default `1`) â€” write `config/generated/*`
- `OFFLINE`: `1|0` â€” skip network expectations
- `VSCODE_AI`: `gemini|codex|off` â€” inline suggestions provider in VS Code
- `PROJECT_NAME`: used in README + notebook title

---

## ğŸ§¯ Troubleshooting

- Conda not initialized: `source "$(conda info --base)/etc/profile.d/conda.sh"`
- Script not executable: `chmod +x deploy_toolkit/scripts/bootstrap.sh`
- Multiple CSVs: use `DATASET=prompt` or pass a path with `DATASET=/path/to.csv`
- Notebook wonâ€™t open via `make notebook`: ensure Jupyter is installed in `<ENV>` or run `jupyter lab notebooks/toolkit_template.ipynb`

---

## ğŸ”’ Privacy Notes

- Config inference is fully local: no data leaves your machine
- The toolkit avoids logging raw rows; outputs are summaries, reports, and dashboards
- `.gitignore` excludes `data/**` and `exports/**` by default

---

<p align="center">
  <a href="../README.md">ğŸ  <b>Main README</b></a>
  &nbsp;Â·&nbsp;
  <a href="./deployment_guide.md">ğŸš€ <b>Deployment</b></a>
  &nbsp;Â·&nbsp;
  <a href="./usage_guide.md">ğŸ“˜ <b>Usage</b></a>
  &nbsp;Â·&nbsp;
  <a href="./config_guide.md">ğŸ§­ <b>Config</b></a>
  &nbsp;Â·&nbsp;
  <a href="./notebook_usage_guide.md">ğŸ“— <b>Notebooks</b></a>
</p>
<p align="center">
  <sub>âœ¨ Analyst Toolkit Â· Starter Kit âœ¨</sub>
</p>
